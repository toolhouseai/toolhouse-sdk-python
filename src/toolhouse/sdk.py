# This file was generated by liblab | https://liblab.com/

"""
Creates a Toolhouse class.
Generates the main SDK with all available queries as attributes.

Class:
    Toolhouse
"""
import os
from enum import Enum
from typing import Any, Dict, List, Optional, Union
from warnings import warn

from .exceptions import ToolhouseError
from .models.GetToolsRequest import GetToolsRequest
from .models.Provider import Provider as ProviderModel
from .models.RunToolsRequest import RunToolsRequest
from .models.Stream import GroqStream, OpenAIStream, ToolhouseStreamStorage, stream_to_chat_completion
from .net.environment import Environment
from .services.local_tools import LocalTools
from .services.tools import Tools


class MissingDependencyError(ImportError):
    """Raised when an optional dependency is not installed."""

    pass


class Toolhouse:
    """
    A class representing the full Toolhouse SDK

    Attributes
    ----------
    tools : Tools

    Methods
    -------
    set_base_url(url: str)
        Sets the end URL
    set_api_key(api_key)
        Set the api_key
    """

    @property
    def llama_index(self):
        """LlamaIndex integration, requires toolhouse[llamaindex] extra."""
        try:
            from .services.llamaindex import LlamaIndex

            if self._llama_index is None:
                self._llama_index = LlamaIndex(self.tools)
            return self._llama_index
        except ImportError as e:
            raise MissingDependencyError(
                "LlamaIndex integration requires extra dependencies. "
                "Install them with: pip install toolhouse[llamaindex]"
            ) from e

    def __init__(
        self,
        api_key: Optional[str] = None,
        provider: Union[ProviderModel, str] = ProviderModel.OPENAI,
        environment: Environment = Environment.DEFAULT,
        access_token: Optional[str] = None,
    ) -> None:
        """
        Initializes the Toolhouse SDK class.

        Parameters
        ----------
        api_key : str
            The API key
        provider : ProviderModel
            The provider model or name
        environment : Environment
            The environment that the SDK is accessing
        """
        if not provider:
            raise ValueError("Parameter provider is required, cannot be empty or blank.")
        self.provider = self._enum_matching(provider, ProviderModel.list(), "provider")
        if access_token is not None and api_key is None:
            warn("access_token property will be deprecated on next major release, please use api_key instead")
            api_key = access_token
        if access_token is None and api_key is None:
            api_key = os.environ.get("TOOLHOUSE_API_KEY", None)
        if api_key is None:
            raise ToolhouseError(
                "The api_key client option must be set either by passing api_key to the SDK or by setting the TOOLHOUSE_API_KEY environment variable"
            )
        self.api_key = api_key
        self.tools = Tools(api_key)
        self.metadata: Dict[str, Any] = {}
        self.set_base_url(environment.value if isinstance(environment, Environment) else environment)
        self.local_tools: LocalTools = LocalTools()
        self._llama_index = None  # Initialize as None
        self.bundle: str = "default"

    def register_local_tool(self, local_tool):
        """Register Local Tools"""
        return self.local_tools.register_local_tool(local_tool)

    def set_metadata(self, key: str, value) -> None:
        """
        Sets User Metadata

        Parameters
        ----------
            key
            value
        """
        self.metadata[key] = value

    def set_provider(self, provider: ProviderModel) -> None:
        """
        Sets User Metadata

        Parameters
        ----------
        provider : ProviderModel
            The provider model or name
        """
        self.provider = self._enum_matching(provider, ProviderModel.list(), "provider")

    def set_base_url(self, url: str) -> None:
        """
        Sets the end URL

        Parameters
        ----------
            url:
                The end URL
        """
        self.tools.set_base_url(url)

    def set_access_token(self, token: str) -> None:
        """
        Sets auth token key

        Parameters
        ----------
        token: string
            Auth token value
        """
        warn("This method will be deprecated please use set_api_key instead")
        self.tools.set_api_key(token)

    def set_api_key(self, api_key: str) -> None:
        """
        Sets api key

        Parameters
        ----------
        api_key: string
            Api key value
        """
        self.tools.set_api_key(api_key)

    def get_tools(self, bundle="default"):
        """
        Get Tools
        """
        self.bundle = bundle
        request = GetToolsRequest(provider=self.provider, metadata=self.metadata, bundle=bundle)
        tools = self.tools.get_tools(request)
        if self.provider in ("llamaindex", ProviderModel.LLAMAINDEX):
            return self.llama_index.get_tools(tools, request)
        return tools

    def run_tools(self, response, append: bool = True) -> List:
        """
        Run Tools based on the response.
        Parameters
        ----------
        response : Any
            The response from the provider
        append : bool
            Appends the LLM response to the list of messages in the return value.

        Returns
        -------
        List
            A list of messages
        """
        messages: List = []

        if self.provider in ("openai", ProviderModel.OPENAI):
            if isinstance(response, (ToolhouseStreamStorage, GroqStream, OpenAIStream)):
                response = stream_to_chat_completion(response)
                if response is None:
                    return []
            if response.choices[0].finish_reason != "tool_calls":
                return []
            response_message = response.choices[0].message
            # Strip audio and refusal if they're none to support compatibility with other models
            if hasattr(response.choices[0].message, "audio") and response.choices[0].message.audio is None:
                del response.choices[0].message.audio
            if hasattr(response.choices[0].message, "refusal") and response.choices[0].message.refusal is None:
                del response.choices[0].message.refusal

            if append:
                msg = response_message.model_dump()
                del msg["function_call"]
                messages.append(msg)
            tool_calls = getattr(response_message, "tool_calls", None)

            if tool_calls:
                for tool in tool_calls:
                    if tool.function.name in self.local_tools.get_registered_tools():
                        result = self.local_tools.run_tools(tool)
                        messages.append(result.model_dump())
                    else:
                        run_tool_request = RunToolsRequest(tool, self.provider, self.metadata, self.bundle)
                        run_response = self.tools.run_tools(run_tool_request)
                        messages.append(run_response.content)

        elif self.provider in ("anthropic", ProviderModel.ANTHROPIC):
            if response.stop_reason != "tool_use":
                return []

            message: dict = {"role": "user", "content": []}
            for tool in response.content:
                if tool.type == "tool_use":
                    if tool.name in self.local_tools.get_registered_tools():
                        result = self.local_tools.run_tools(tool)
                        message["content"].append(result.model_dump())
                    else:
                        run_tool_request = RunToolsRequest(tool, self.provider, self.metadata, self.bundle)
                        run_response = self.tools.run_tools(run_tool_request)
                        output = run_response.content
                        message["content"].append(output)
            if message["content"]:
                if append:
                    messages.append({"role": "assistant", "content": response.content})
                messages.append(message)

        else:
            raise NotImplementedError("Provider not supported")

        return messages

    @classmethod
    def _enum_matching(cls, value: Union[str, Enum], enum_values: List[str], variable_name: str):
        str_value = value.value if isinstance(value, Enum) else value
        if str_value in enum_values:
            return value
        else:
            raise ValueError(f"Invalid value for {variable_name}: must match one of {enum_values}")
